{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fb0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import pickle\n",
    "\n",
    "def defaultdict_to_dict(d):\n",
    "    \"\"\" Recursively convert defaultdict to dict. \"\"\"\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {key: defaultdict_to_dict(value) for key, value in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786849ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_files_with_str(directory, str_contain, index):\n",
    "    # Create an empty list to store files that contain str_contain\n",
    "    files_with_str = []\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file[:4] == str_contain:\n",
    "            files_with_str.append(file)\n",
    "    \n",
    "    \n",
    "    # Sort only the files that contain 'x1'\n",
    "    files_with_str.sort()\n",
    "\n",
    "    # Return the first file in the sorted list, or None if the list is empty\n",
    "    return files_with_str[index] if files_with_str else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0865bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got here 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8090 8090_2023-12-16 175932_my_t5_model\n",
      "1 8091 8091_2023-12-16 180341_my_t5_model\n",
      "1 8092 8092_2023-12-16 180103_my_t5_model\n",
      "1 8093 8093_2023-12-16 191901_my_t5_model\n",
      "1 8094 8094_2023-12-16 180320_my_t5_model\n",
      "1 8095 8095_2023-12-16 180348_my_t5_model\n",
      "1 8096 8096_2023-12-16 180129_my_t5_model\n",
      "1 8097 8097_2023-12-16 180007_my_t5_model\n",
      "1 8098 8098_2023-12-16 191911_my_t5_model\n",
      "1 8099 8099_2023-12-16 191700_my_t5_model\n",
      "2 8090 8090_2023-12-17 123459_my_t5_model\n",
      "2 8091 8091_2023-12-17 123536_my_t5_model\n",
      "2 8092 8092_2023-12-17 123202_my_t5_model\n",
      "2 8093 8093_2023-12-17 123152_my_t5_model\n",
      "2 8094 8094_2023-12-17 124010_my_t5_model\n",
      "2 8095 8095_2023-12-17 123723_my_t5_model\n",
      "2 8096 8096_2023-12-17 140032_my_t5_model\n",
      "2 8097 8097_2023-12-17 123518_my_t5_model\n",
      "2 8098 8098_2023-12-17 123711_my_t5_model\n",
      "2 8099 8099_2023-12-17 123633_my_t5_model\n",
      "3 8090 8090_2023-12-18 082034_my_t5_model\n",
      "3 8091 8091_2023-12-18 070742_my_t5_model\n",
      "3 8092 8092_2023-12-18 070621_my_t5_model\n",
      "3 8093 8093_2023-12-18 081950_my_t5_model\n",
      "3 8094 8094_2023-12-18 071001_my_t5_model\n",
      "3 8095 8095_2023-12-18 070540_my_t5_model\n",
      "3 8096 8096_2023-12-18 070510_my_t5_model\n",
      "3 8097 8097_2023-12-18 070723_my_t5_model\n",
      "3 8098 8098_2023-12-18 070705_my_t5_model\n",
      "3 8099 8099_2023-12-18 070413_my_t5_model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Reading data and models\n",
    "groups = [str(i) for i in range(1,4)]\n",
    "peers = [str(i) for i in range(8090, 8100)]\n",
    "\n",
    "#reading entire dataset for all groups: train_df_group1\n",
    "\n",
    "\n",
    "\n",
    "# for group in groups:\n",
    "#     datasets_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}','datasets')\n",
    "#     personal_dfs = os.listdir(datasets_folder)\n",
    "#     personal_dfs = [i for i in personal_dfs if '.csv' in i and 'train' not in i and 'test' not in i]\n",
    "#     print (personal_dfs)\n",
    "#     for i in personal_dfs:\n",
    "#         df_temp = pd.read_csv(os.path.join(datasets_folder, i))\n",
    "#         print (df_temp['doc_id'].nunique())\n",
    "#     exec(f'train_df_group{group} = train_df_group{group}.drop_duplicates()')\n",
    "    \n",
    "    \n",
    "# reading individual peer datasets & group datasets: train_df_group1, train_df_group1_peer1\n",
    "for group in groups:\n",
    "    # creating train_df's\n",
    "    exec(f'train_df_group{group} = pd.DataFrame()')\n",
    "    for peer in peers:\n",
    "        datasets_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}','datasets')\n",
    "        exec_str = f\"train_df_group{group}_peer{int(peer) - 8089} = pd.read_csv(os.path.join(datasets_folder,'{peer}_df.csv'))\"\n",
    "        exec(exec_str)\n",
    "        exec(f'train_df_group{group} = pd.concat([train_df_group{group}, train_df_group{group}_peer{int(peer) - 8089}])')\n",
    "       \n",
    "    exec(f'train_df_group{group} = train_df_group{group}.drop_duplicates()')\n",
    "    \n",
    "    # creating test_df's: test_df_group1, test_df_group1_peer1\n",
    "    \n",
    "    datasets_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}','datasets')\n",
    "    exec (f\"test_df_group{group} = pd.read_csv(os.path.join(datasets_folder,'test_df.csv')) \")\n",
    "    exec (f\"test_df_group{group} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}['doc_id'].unique())]\")\n",
    "    for peer in peers:\n",
    "        exec (f\"test_df_group{group}_peer{int(peer) - 8089} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}_peer{int(peer) - 8089}['doc_id'].unique())]\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#reading models: model_group1_peer1\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        model_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}', 'models')\n",
    "        model_file = find_first_files_with_str(model_folder, peer, 10) # 10 is the largest number of saved models that all peers have finished training\n",
    "        print (group, peer, model_file)\n",
    "        exec_str = f\"model_group{group}_peer{str(int(peer)-8089)} = T5ForConditionalGeneration.from_pretrained(os.path.join(model_folder, model_file))\"\n",
    "        \n",
    "        \n",
    "        exec(exec_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11afe6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_group2['doc_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745af642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "2032\n",
      "2002\n"
     ]
    }
   ],
   "source": [
    "print (train_df_group1['doc_id'].nunique())\n",
    "print (train_df_group2['doc_id'].nunique())\n",
    "print (train_df_group3['doc_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a57d2",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 5 beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4d5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_group1_list = []\n",
    "models_group2_list = []\n",
    "models_group3_list = []\n",
    "\n",
    "for group in groups:\n",
    "    for i, peer in enumerate(peers):\n",
    "        exec(f'models_group{group}_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f86555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:train set size: 16432\n",
      "test set size: 16464\n",
      " 17311\n",
      "test set size: 17329\n",
      "train set size: 16647\n",
      "test set size: 16674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 17000 queries\n",
      "Processed 17000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 17000 queries\n",
      "Processed 18000 queries\n",
      "Processed 18000 queries\n",
      "Processed 18000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, train_df, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.train_df = train_df.copy()\n",
    "        self.test_df = test_df.copy()\n",
    "        \n",
    "        print ('train set size:', self.train_df.shape[0])\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        sampled_models = random.sample(self.model_list, 5)\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5)\n",
    "\n",
    "#             beam_scores = output.sequences_scores\n",
    "#             probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            model_res = list(set(model_res))\n",
    "        \n",
    "            for res in model_res:\n",
    "                results[res] += 1\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:5]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "        acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    \n",
    "    \n",
    "    model_list = globals()[f'models_group{group_nbr}_list']\n",
    "    train_df = globals()[f'train_df_group{group_nbr}']\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, train_df, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_train, acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_train': acc_train, 'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Train Acc: {acc_train}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "    thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0e4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('accuracies_samplingmodels_5beams_intragroup.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d98e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: {'acc_train': 0.9999399291163573, 'acc_test': 0.9535204510015594},\n",
       " 1: {'acc_train': 0.9996957156767283, 'acc_test': 0.9482507288629738},\n",
       " 2: {'acc_train': 0.9995378660967015, 'acc_test': 0.9451786023428934}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('accuracies_samplingmodels_5beams_intragroup.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d88a2",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 5 beams, with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f25364",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_group1_list = []\n",
    "models_group2_list = []\n",
    "models_group3_list = []\n",
    "\n",
    "for group in groups:\n",
    "    for i, peer in enumerate(peers):\n",
    "        exec(f'models_group{group}_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5eaaf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 16432\n",
      "test set size: 16464\n",
      "train set size: 16647\n",
      "test set size: 16674\n",
      "train set size: 17311\n",
      "test set size: 17329\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 17000 queries\n",
      "Processed 17000 queries\n",
      "Processed 17000 queries\n",
      "Processed 18000 queries\n",
      "Processed 18000 queries\n",
      "Processed 18000 queries\n",
      "Processed 19000 queries\n",
      "Processed 19000 queries\n",
      "Processed 19000 queries\n",
      "Processed 20000 queries\n",
      "Processed 20000 queries\n",
      "Processed 20000 queries\n",
      "Processed 21000 queries\n",
      "Processed 21000 queries\n",
      "Processed 21000 queries\n",
      "Processed 22000 queries\n",
      "Processed 22000 queries\n",
      "Processed 22000 queries\n",
      "Processed 23000 queries\n",
      "Processed 23000 queries\n",
      "Processed 23000 queries\n",
      "Processed 24000 queries\n",
      "Processed 24000 queries\n",
      "Processed 24000 queries\n",
      "Processed 25000 queries\n",
      "Processed 25000 queries\n",
      "Processed 25000 queries\n",
      "Processed 26000 queries\n",
      "Processed 26000 queries\n",
      "Processed 26000 queries\n",
      "Processed 27000 queries\n",
      "Processed 27000 queries\n",
      "Processed 27000 queries\n",
      "Processed 28000 queries\n",
      "Processed 28000 queries\n",
      "Processed 28000 queries\n",
      "Processed 29000 queries\n",
      "Processed 29000 queries\n",
      "Processed 29000 queries\n",
      "Processed 30000 queries\n",
      "Processed 30000 queries\n",
      "Processed 30000 queries\n",
      "Processed 31000 queries\n",
      "Processed 31000 queries\n",
      "Processed 31000 queries\n",
      "Processed 32000 queries\n",
      "Processed 32000 queries\n",
      "Processed 32000 queries\n",
      "Processed 33000 queries\n",
      "Processed 33000 queries\n",
      "Group: 3, Train Acc: 0.9996996455817865, Test Acc: 0.9497421134700732\n",
      "Group: 1, Train Acc: 0.9995740019474196, Test Acc: 0.9467322643343051\n",
      "Processed 34000 queries\n",
      "Group: 2, Train Acc: 0.9993067991450523, Test Acc: 0.943043453170985\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, train_df, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.train_df = train_df.copy()\n",
    "        self.test_df = test_df.copy()\n",
    "        \n",
    "        print ('train set size:', self.train_df.shape[0])\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        sampled_models = random.sample(self.model_list, 3)\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5, max_length = 20)\n",
    "\n",
    "            beam_scores = output.sequences_scores\n",
    "            probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "#             model_res = list(set(model_res))\n",
    "        \n",
    "            for res, prob in zip(model_res, probabilities):\n",
    "                results[res] += prob\n",
    "\n",
    "        \n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:5]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "        acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    \n",
    "    \n",
    "    model_list = globals()[f'models_group{group_nbr}_list']\n",
    "    train_df = globals()[f'train_df_group{group_nbr}']\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, train_df, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_train, acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_train': acc_train, 'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Train Acc: {acc_train}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "    thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70580685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('accuracies_samplingmodels_5beams_probabilities_intragroup.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e36e2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: {'acc_train': 0.9999399291163573, 'acc_test': 0.9568789732517692},\n",
       " 1: {'acc_train': 0.9998782862706913, 'acc_test': 0.9523809523809523},\n",
       " 2: {'acc_train': 0.9998844665241754, 'acc_test': 0.9489872468117029}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('accuracies_samplingmodels_5beams_probabilities_intragroup.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5140640",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 1 beam, with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c053858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e21b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_group1_list = []\n",
    "models_group2_list = []\n",
    "models_group3_list = []\n",
    "\n",
    "for group in groups:\n",
    "    for i, peer in enumerate(peers):\n",
    "        exec(f'models_group{group}_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba2888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 16432\n",
      "test set size: 16464\n",
      "train set size: 17311\n",
      "test set size: 17329\n",
      "train set size: 16647\n",
      "test set size: 16674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 17000 queries\n",
      "Processed 17000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 17000 queries\n",
      "Processed 18000 queries\n",
      "Processed 18000 queries\n",
      "Processed 18000 queries\n",
      "Processed 19000 queries\n",
      "Processed 19000 queries\n",
      "Processed 19000 queries\n",
      "Processed 20000 queries\n",
      "Processed 20000 queries\n",
      "Processed 20000 queries\n",
      "Processed 21000 queries\n",
      "Processed 21000 queries\n",
      "Processed 21000 queries\n",
      "Processed 22000 queries\n",
      "Processed 22000 queries\n",
      "Processed 22000 queries\n",
      "Processed 23000 queries\n",
      "Processed 23000 queries\n",
      "Processed 23000 queries\n",
      "Processed 24000 queries\n",
      "Processed 24000 queries\n",
      "Processed 24000 queries\n",
      "Processed 25000 queries\n",
      "Processed 25000 queries\n",
      "Processed 25000 queries\n",
      "Processed 26000 queries\n",
      "Processed 26000 queries\n",
      "Processed 26000 queries\n",
      "Processed 27000 queries\n",
      "Processed 27000 queries\n",
      "Processed 27000 queries\n",
      "Processed 28000 queries\n",
      "Processed 28000 queries\n",
      "Processed 28000 queries\n",
      "Processed 29000 queries\n",
      "Processed 29000 queries\n",
      "Processed 29000 queries\n",
      "Processed 30000 queries\n",
      "Processed 30000 queries\n",
      "Processed 30000 queries\n",
      "Processed 31000 queries\n",
      "Processed 31000 queries\n",
      "Processed 31000 queries\n",
      "Processed 32000 queries\n",
      "Processed 32000 queries\n",
      "Processed 32000 queries\n",
      "Processed 33000 queries\n",
      "Processed 33000 queries\n",
      "Group: 3, Train Acc: 0.9967561722832943, Test Acc: 0.9292311382991484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: 1, Train Acc: 0.9956791626095424, Test Acc: 0.9207968901846453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 34000 queries\n",
      "Group: 2, Train Acc: 0.9965339957252614, Test Acc: 0.9186335045299786\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, train_df, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.train_df = train_df.copy()\n",
    "        self.test_df = test_df.copy()\n",
    "        \n",
    "        print ('train set size:', self.train_df.shape[0])\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        sampled_models = random.sample(self.model_list, 5)\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5)\n",
    "\n",
    "            beam_scores = output.sequences_scores\n",
    "            probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "#             model_res = list(set(model_res))\n",
    "        \n",
    "            for res, prob in zip(model_res, probabilities):\n",
    "                results[res] += prob\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:1]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "        acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    \n",
    "    \n",
    "    model_list = globals()[f'models_group{group_nbr}_list']\n",
    "    train_df = globals()[f'train_df_group{group_nbr}']\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, train_df, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_train, acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_train': acc_train, 'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Train Acc: {acc_train}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "    thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "471c8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_beam = defaultdict(lambda: defaultdict(float))\n",
    "for i in model_managers:\n",
    "\n",
    "    acc_test = model_managers[i].test_df.apply(\n",
    "        lambda row: row['doc_id'] in row['generated_doc_id'][:1], axis=1).sum() / model_managers[i].test_df.shape[0]\n",
    "    acc_train = model_managers[i].train_df.apply(\n",
    "        lambda row: row['doc_id'] in row['generated_doc_id'][:1], axis=1).sum() / model_managers[i].test_df.shape[0]\n",
    "              \n",
    "    one_beam[i]['acc_test'] = acc_test\n",
    "    one_beam[i]['acc_train'] = acc_train\n",
    "\n",
    "five_beams = defaultdict(lambda: defaultdict(float))\n",
    "for i in model_managers:\n",
    "\n",
    "    acc_test = model_managers[i].test_df.apply(\n",
    "        lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / model_managers[i].test_df.shape[0]\n",
    "    acc_train = model_managers[i].train_df.apply(\n",
    "        lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / model_managers[i].test_df.shape[0]\n",
    "    \n",
    "    five_beams[i]['acc_test'] = acc_test\n",
    "    five_beams[i]['acc_train'] = acc_train\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97fbf70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {1: defaultdict(float,\n",
       "                         {'acc_test': 0.9467322643343051,\n",
       "                          'acc_train': 0.997631195335277}),\n",
       "             3: defaultdict(float,\n",
       "                         {'acc_test': 0.9497421134700732,\n",
       "                          'acc_train': 0.9980808444284515}),\n",
       "             2: defaultdict(float,\n",
       "                         {'acc_test': 0.943043453170985,\n",
       "                          'acc_train': 0.9982687979687229})})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aadc970b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {1: defaultdict(float,\n",
       "                         {'acc_test': 0.9129616132167152,\n",
       "                          'acc_train': 0.9923469387755102}),\n",
       "             3: defaultdict(float,\n",
       "                         {'acc_test': 0.9224541201871177,\n",
       "                          'acc_train': 0.9939426652273}),\n",
       "             2: defaultdict(float,\n",
       "                         {'acc_test': 0.9121126435455018,\n",
       "                          'acc_train': 0.9938830861561544})})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ccf69b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "one_beam = defaultdict_to_dict(one_beam)\n",
    "five_beams = defaultdict_to_dict(five_beams)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('accuracies_samplingmodels_1beam_probabilities_intragroup.pkl', 'wb') as file:\n",
    "    pickle.dump(one_beam, file)\n",
    "    \n",
    "# Serialize and save to a file\n",
    "with open('accuracies_samplingmodels_5beams_probabilities_intragroup.pkl', 'wb') as file:\n",
    "    pickle.dump(five_beams, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7162bc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: {'acc_train': 0.9996996455817865, 'acc_test': 0.9497421134700732},\n",
       " 1: {'acc_train': 0.9995740019474196, 'acc_test': 0.9467322643343051},\n",
       " 2: {'acc_train': 0.9993067991450523, 'acc_test': 0.943043453170985}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4431ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accuracies_samplingmodels_1beam_probabilities_intragroup.pkl', 'rb') as file:\n",
    "    loaded_dict = five_beams.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8c6b6",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 1 beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab43288",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_group1_list = []\n",
    "models_group2_list = []\n",
    "models_group3_list = []\n",
    "\n",
    "for group in groups:\n",
    "    for i, peer in enumerate(peers):\n",
    "        exec(f'models_group{group}_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "459bb30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:train set size: 16647\n",
      "test set size: 16674\n",
      " 16432\n",
      "test set size: 16464\n",
      "train set size: 17311\n",
      "test set size: 17329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 18000 queries\n",
      "Processed 18000 queries\n",
      "Processed 18000 queries\n",
      "Processed 20000 queries\n",
      "Processed 20000 queries\n",
      "Processed 20000 queries\n",
      "Processed 22000 queries\n",
      "Processed 22000 queries\n",
      "Processed 22000 queries\n",
      "Processed 24000 queries\n",
      "Processed 24000 queries\n",
      "Processed 24000 queries\n",
      "Processed 26000 queries\n",
      "Processed 26000 queries\n",
      "Processed 26000 queries\n",
      "Processed 28000 queries\n",
      "Processed 28000 queries\n",
      "Processed 28000 queries\n",
      "Processed 30000 queries\n",
      "Processed 30000 queries\n",
      "Processed 30000 queries\n",
      "Processed 32000 queries\n",
      "Processed 32000 queries\n",
      "Processed 32000 queries\n",
      "Group: 3, Train Acc: 0.9962756052141527, Test Acc: 0.9232337771380592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: 1, Train Acc: 0.9939143135345667, Test Acc: 0.915877065111759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 34000 queries\n",
      "Group: 2, Train Acc: 0.9951475940153659, Test Acc: 0.9151133937330487\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, train_df, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.train_df = train_df.copy()\n",
    "        self.test_df = test_df.copy()\n",
    "        \n",
    "        print ('train set size:', self.train_df.shape[0])\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 2000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        sampled_models = random.sample(self.model_list, 5)\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5)\n",
    "\n",
    "#             beam_scores = output.sequences_scores\n",
    "#             probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "#             model_res = list(set(model_res))\n",
    "        \n",
    "            for res in model_res:\n",
    "                results[res] += 1\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:1]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "        acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    \n",
    "    \n",
    "    model_list = globals()[f'models_group{group_nbr}_list']\n",
    "    train_df = globals()[f'train_df_group{group_nbr}']\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, train_df, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_train, acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_train': acc_train, 'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Train Acc: {acc_train}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "    thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7b1feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('accuracies_samplingmodels_1beam_intragroup.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "162aa45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: {'acc_train': 0.9962756052141527, 'acc_test': 0.9232337771380592},\n",
       " 1: {'acc_train': 0.9939143135345667, 'acc_test': 0.915877065111759},\n",
       " 2: {'acc_train': 0.9951475940153659, 'acc_test': 0.9151133937330487}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('accuracies_samplingmodels_1beam_intragroup.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a89a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
