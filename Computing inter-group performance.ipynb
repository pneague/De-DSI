{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fb0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import pickle\n",
    "import torch\n",
    "def defaultdict_to_dict(d):\n",
    "    \"\"\" Recursively convert defaultdict to dict. \"\"\"\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {key: defaultdict_to_dict(value) for key, value in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786849ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_files_with_str(directory, str_contain, index):\n",
    "    # Create an empty list to store files that contain str_contain\n",
    "    files_with_str = []\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file[:4] == str_contain:\n",
    "            files_with_str.append(file)\n",
    "    \n",
    "    \n",
    "    # Sort only the files that contain 'x1'\n",
    "    files_with_str.sort()\n",
    "\n",
    "    # Return the first file in the sorted list, or None if the list is empty\n",
    "    return files_with_str[index] if files_with_str else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0865bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got here 2\n",
      "1 8090 8090_2023-12-16 175932_my_t5_model\n",
      "1 8091 8091_2023-12-16 180341_my_t5_model\n",
      "1 8092 8092_2023-12-16 180103_my_t5_model\n",
      "1 8093 8093_2023-12-16 191901_my_t5_model\n",
      "1 8094 8094_2023-12-16 180320_my_t5_model\n",
      "1 8095 8095_2023-12-16 180348_my_t5_model\n",
      "1 8096 8096_2023-12-16 180129_my_t5_model\n",
      "1 8097 8097_2023-12-16 180007_my_t5_model\n",
      "1 8098 8098_2023-12-16 191911_my_t5_model\n",
      "1 8099 8099_2023-12-16 191700_my_t5_model\n",
      "2 8090 8090_2023-12-17 123459_my_t5_model\n",
      "2 8091 8091_2023-12-17 123536_my_t5_model\n",
      "2 8092 8092_2023-12-17 123202_my_t5_model\n",
      "2 8093 8093_2023-12-17 123152_my_t5_model\n",
      "2 8094 8094_2023-12-17 124010_my_t5_model\n",
      "2 8095 8095_2023-12-17 123723_my_t5_model\n",
      "2 8096 8096_2023-12-17 140032_my_t5_model\n",
      "2 8097 8097_2023-12-17 123518_my_t5_model\n",
      "2 8098 8098_2023-12-17 123711_my_t5_model\n",
      "2 8099 8099_2023-12-17 123633_my_t5_model\n",
      "3 8090 8090_2023-12-18 082034_my_t5_model\n",
      "3 8091 8091_2023-12-18 070742_my_t5_model\n",
      "3 8092 8092_2023-12-18 070621_my_t5_model\n",
      "3 8093 8093_2023-12-18 081950_my_t5_model\n",
      "3 8094 8094_2023-12-18 071001_my_t5_model\n",
      "3 8095 8095_2023-12-18 070540_my_t5_model\n",
      "3 8096 8096_2023-12-18 070510_my_t5_model\n",
      "3 8097 8097_2023-12-18 070723_my_t5_model\n",
      "3 8098 8098_2023-12-18 070705_my_t5_model\n",
      "3 8099 8099_2023-12-18 070413_my_t5_model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Reading data and models\n",
    "groups = [str(i) for i in range(1,4)]\n",
    "peers = [str(i) for i in range(8090, 8100)]\n",
    "\n",
    "#reading entire dataset for all groups: train_df_group1\n",
    "\n",
    "\n",
    "\n",
    "# for group in groups:\n",
    "#     datasets_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}','datasets')\n",
    "#     personal_dfs = os.listdir(datasets_folder)\n",
    "#     personal_dfs = [i for i in personal_dfs if '.csv' in i and 'train' not in i and 'test' not in i]\n",
    "#     print (personal_dfs)\n",
    "#     for i in personal_dfs:\n",
    "#         df_temp = pd.read_csv(os.path.join(datasets_folder, i))\n",
    "#         print (df_temp['doc_id'].nunique())\n",
    "#     exec(f'train_df_group{group} = train_df_group{group}.drop_duplicates()')\n",
    "    \n",
    "    \n",
    "# reading individual peer datasets & group datasets: train_df_group1, train_df_group1_peer1\n",
    "for group in groups:\n",
    "    # creating train_df's\n",
    "    exec(f'train_df_group{group} = pd.DataFrame()')\n",
    "    for peer in peers:\n",
    "        datasets_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}','datasets')\n",
    "        exec_str = f\"train_df_group{group}_peer{int(peer) - 8089} = pd.read_csv(os.path.join(datasets_folder,'{peer}_df.csv'))\"\n",
    "        exec(exec_str)\n",
    "        exec(f'train_df_group{group} = pd.concat([train_df_group{group}, train_df_group{group}_peer{int(peer) - 8089}])')\n",
    "       \n",
    "    exec(f'train_df_group{group} = train_df_group{group}.drop_duplicates()')\n",
    "    \n",
    "    # creating test_df's: test_df_group1, test_df_group1_peer1\n",
    "    \n",
    "    datasets_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}','datasets')\n",
    "    exec (f\"test_df_group{group} = pd.read_csv(os.path.join(datasets_folder,'test_df.csv')) \")\n",
    "    exec (f\"test_df_group{group} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}['doc_id'].unique())]\")\n",
    "    for peer in peers:\n",
    "        exec (f\"test_df_group{group}_peer{int(peer) - 8089} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}_peer{int(peer) - 8089}['doc_id'].unique())]\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#reading models: model_group1_peer1\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        model_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}', 'models')\n",
    "        model_file = find_first_files_with_str(model_folder, peer, 10) # 10 is the largest number of saved models that all peers have finished training\n",
    "        print (group, peer, model_file)\n",
    "        exec_str = f\"model_group{group}_peer{str(int(peer)-8089)} = T5ForConditionalGeneration.from_pretrained(os.path.join(model_folder, model_file))\"\n",
    "        \n",
    "        \n",
    "        exec(exec_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a65048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_group1_peer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b5fc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "2001\n",
      "2032\n",
      "2002\n",
      "test\n",
      "2001\n",
      "2032\n",
      "2002\n"
     ]
    }
   ],
   "source": [
    "print ('train')\n",
    "print (train_df_group1['doc_id'].nunique())\n",
    "print (train_df_group2['doc_id'].nunique())\n",
    "print (train_df_group3['doc_id'].nunique())\n",
    "print ('test')\n",
    "print (test_df_group1['doc_id'].nunique())\n",
    "print (test_df_group2['doc_id'].nunique())\n",
    "print (test_df_group3['doc_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55c8c6",
   "metadata": {},
   "source": [
    "### CALCULATING ACCURACIES ON MODELS' OWN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_and_evaluate(group, peer, mode = 'global'):\n",
    "    global_scope = globals()\n",
    "    acc_train = -1\n",
    "    print ('group ', group, 'peer ', peer)\n",
    "    exec (f'model = model_group{group}_peer{peer}', global_scope)\n",
    "    if mode == 'global':\n",
    "        exec (f'train_df = train_df_group{group}.sample(1000).copy()', global_scope)\n",
    "        exec (f'test_df = test_df_group{group}.sample(1000).copy()', global_scope)\n",
    "    elif mode == 'local':\n",
    "        exec (f'train_df = train_df_group{group}_peer{peer}.sample(1000).copy()', global_scope)\n",
    "        exec (f'test_df = test_df_group{group}_peer{peer}.sample(1000).copy()', global_scope)\n",
    "        \n",
    "    df_tot = train_df.copy()\n",
    "    df_tst = test_df.copy()\n",
    "    print (df_tot.shape, df_tst.shape)\n",
    "    \n",
    "    df_tot['generated_doc_id'] = df_tot['query'].apply(lambda x: generate_text(x, model))\n",
    "    df_tst['generated_doc_id'] = df_tst['query'].apply(lambda x: generate_text(x, model))\n",
    "    acc_train = df_tot[df_tot['doc_id'] == df_tot['generated_doc_id']].shape[0]/df_tot.shape[0]\n",
    "    acc_test = df_tst[df_tst['doc_id'] == df_tst['generated_doc_id']].shape[0]/df_tst.shape[0]\n",
    "    \n",
    "    \n",
    "    print (f'{mode} training set accuracy: ', acc_train)\n",
    "    print (f'{mode} test set accuracy: ', acc_test)\n",
    "    \n",
    "    \n",
    "    df_tot['generated_doc_id_log'] = df_tot['query'].apply(lambda x: generate_text_through_logits(x, model, df_tot))\n",
    "    df_tst['generated_doc_id_log'] = df_tst['query'].apply(lambda x: generate_text_through_logits(x, model, df_tst))\n",
    "\n",
    "    \n",
    "    acc_train_log = df_tot[df_tot['doc_id'] == df_tot['generated_doc_id_log']].shape[0]/df_tot.shape[0]\n",
    "    acc_test_log = df_tst[df_tst['doc_id'] == df_tst['generated_doc_id_log']].shape[0]/df_tst.shape[0]\n",
    "    \n",
    "    print (f'{mode} training set accuracy log: ', acc_train_log)\n",
    "    print (f'{mode} test set accuracy log: ', acc_test_log)\n",
    "    return acc_train, acc_test, df_tot, df_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(query, model):\n",
    "    input_ids = tokenizer.encode(query, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length = 20)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_through_logits(query,model, df_tst):\n",
    "    doc_id = df_tst[df_tst['query'] == query]['doc_id'].iloc[0]\n",
    "#     print (query, doc_id)\n",
    "    inputs = tokenizer(query, padding=False, return_tensors=\"pt\", truncation=True).input_ids\n",
    "    labels = tokenizer(doc_id, padding=True, return_tensors=\"pt\", truncation=True).input_ids\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=inputs, labels = labels)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Extract logits and convert to token IDs\n",
    "    logits = outputs.logits\n",
    "    predicted_token_ids = tokenizer.decode(torch.argmax(logits, dim=-1)[0], skip_special_tokens=True)\n",
    "#     print (predicted_token_ids)\n",
    "    return predicted_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a3ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_accuracies = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        \n",
    "        p = int(peer) - 8089\n",
    "        exec(f\"acc_train_local, acc_test_local, df_tot_l, df_tst_l = read_model_and_evaluate({group}, {p}, 'local')\")\n",
    "        global_accuracies[group][peer]['train']['local'] = acc_train_local\n",
    "        global_accuracies[group][peer]['test']['local'] = acc_test_local\n",
    "        \n",
    "        exec(f\"acc_train_global, acc_test_global, df_tot_g, df_tst_g = read_model_and_evaluate({group}, {p}, 'global')\")\n",
    "        global_accuracies[group][peer]['train']['global'] = acc_train_global\n",
    "        global_accuracies[group][peer]['test']['global'] = acc_test_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc90937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot_g[df_tot_g['generated_doc_id'] != df_tot_g['generated_doc_id_log']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'wb') as file:\n",
    "    dump(global_accuracies, file)\n",
    "\n",
    "# # Step 4: Load from the file\n",
    "# with open('my_defaultdict.pkl', 'rb') as file:\n",
    "#     loaded_defaultdict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a643fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)\n",
    "\n",
    "# To load and optionally convert back to defaultdict\n",
    "# (You'll need to redefine your defaultdict structure as before)\n",
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load and optionally convert back to defaultdict\n",
    "# (You'll need to redefine your defaultdict structure as before)\n",
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'rb') as file:\n",
    "    global_accuracies = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56730e20",
   "metadata": {},
   "source": [
    "### CALCULATING ACCURACIES ON TOP5 FOR EACH PEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_group1.shape,test_df_group2.shape,test_df_group3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c94c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Global dictionary to store models for each group and peer\n",
    "global_objects = {}\n",
    "global_accuracies = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, group, peer):\n",
    "        self.group = group\n",
    "        self.peer = peer\n",
    "        self.counter = 0\n",
    "\n",
    "    def read_model_and_evaluate(self, mode='global'):\n",
    "        global global_accuracies\n",
    "        acc_train = -1.0\n",
    "        acc_test = -1.0\n",
    "        print('group', self.group, 'peer', self.peer, 'mode', mode)\n",
    "\n",
    "        model = globals()[f'model_group{self.group}_peer{self.peer}']\n",
    "        \n",
    "        if mode == 'global':\n",
    "            df_tot = globals()[f'train_df_group{group}'].copy()\n",
    "            self.df_tst = globals()[f'test_df_group{group}'].copy()\n",
    "        elif mode == 'local':\n",
    "            df_tot = globals()[f'train_df_group{group}_peer{peer}'].copy()\n",
    "            self.df_tst = globals()[f'test_df_group{group}_peer{peer}'].copy()\n",
    "        \n",
    "        print(df_tot.shape, self.df_tst.shape)\n",
    "\n",
    "        self.df_tst['generated_doc_id'] = self.df_tst['query'].apply(lambda x: self.generate_text_beams(x, model))\n",
    "        acc_test = self.df_tst.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / self.df_tst.shape[0]\n",
    "        \n",
    "        \n",
    "        global global_objects\n",
    "        global_objects[(self.group, self.peer)] = self.df_tst\n",
    "        \n",
    "        print(f'{mode} training set accuracy: ', acc_train)\n",
    "        print(f'{mode} test set accuracy: ', acc_test)\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def generate_text_beams(self, query, model):\n",
    "        self.counter += 1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        input_ids = tokenizer.encode(query, return_tensors='pt')\n",
    "        output = model.generate(input_ids, do_sample=False, max_length=20,\n",
    "                                num_beams=5, num_return_sequences=5)\n",
    "        return [tokenizer.decode(i, skip_special_tokens=True) for i in output]\n",
    "\n",
    "    def thread_function(self):\n",
    "        global global_accuracies_20samples\n",
    "        acc_train_global, acc_test_global = self.read_model_and_evaluate('global')\n",
    "        global_accuracies[self.group][self.peer]['train']['global'] = acc_train_global\n",
    "        global_accuracies[self.group][self.peer]['test']['global'] = acc_test_global\n",
    "        print(f'finished global work for group {self.group} and peer {self.peer}, acc test global :{acc_test_global}')\n",
    "\n",
    "def evaluate_in_thread(group, peer):\n",
    "    try:\n",
    "        evaluator = ModelEvaluator(group, peer)\n",
    "        evaluator.thread_function()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in thread for group {group} and peer {peer}: {e}\")\n",
    "\n",
    "    \n",
    "# Start threads directly in the main script body\n",
    "threads = []\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        p = int(peer) - 8089\n",
    "        thread = threading.Thread(target=evaluate_in_thread, args=(group, p,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_5beams.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bcdeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_5beams.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a57d2",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 5 beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_groups_list = defaultdict(list)\n",
    "for group in groups:\n",
    "    print (f'group {group}')\n",
    "    for i, peer in enumerate(peers):\n",
    "        print (f'peer {int(peer) - 8089}')\n",
    "        exec(f'three_groups_list[\"group{group}\"].append(model_group{group}_peer{int(peer)-8089})')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in three_groups_list:\n",
    "    print (len(three_groups_list[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86555f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.test_df = test_df.copy()\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 500 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        \n",
    "        sampled_models = random.sample(self.model_list['group1'], 3)\n",
    "        sampled_models.extend(random.sample(self.model_list['group2'], 3))\n",
    "        sampled_models.extend(random.sample(self.model_list['group3'], 3))\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5, max_length = 20)\n",
    "\n",
    "#             beam_scores = output.sequences_scores\n",
    "#             probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            \n",
    "            # sometimes beam-search retrieves two identical strings with different special tokens, filter out these possibilities\n",
    "            model_res = list(set(model_res)) \n",
    "            \n",
    "            for res in model_res:\n",
    "                results[res] += 1\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:5]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "#         self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "#         acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    model_list = three_groups_list\n",
    "\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "        thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e93e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_samplingmodels_5beams.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d98e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_samplingmodels_5beams.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a788bb",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 1 beam - main suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86caf104",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_groups_list = defaultdict(list)\n",
    "for group in groups:\n",
    "    print (f'group {group}')\n",
    "    for i, peer in enumerate(peers):\n",
    "        print (f'peer {int(peer) - 8089}')\n",
    "        exec(f'three_groups_list[\"group{group}\"].append(model_group{group}_peer{int(peer)-8089})')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ee503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.test_df = test_df.copy()\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        self.query = query\n",
    "        if self.counter % 500 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        \n",
    "        sampled_models = random.sample(self.model_list['group1'], 3)\n",
    "        sampled_models.extend(random.sample(self.model_list['group2'], 3))\n",
    "        sampled_models.extend(random.sample(self.model_list['group3'], 3))\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=1, num_return_sequences=1, max_length = 20)\n",
    "            \n",
    "\n",
    "#             beam_scores = output.sequences_scores\n",
    "#             probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            \n",
    "            # sometimes beam-search retrieves two identical strings with different special tokens, filter out these possibilities\n",
    "            model_res = list(set(model_res)) \n",
    "            \n",
    "            for res in model_res:\n",
    "                results[res] += 1\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "#         print ()\n",
    "#         print (self.query)\n",
    "#         print (sorted_responses)\n",
    "#         print (self.test_df[self.test_df['query'] == self.query]['doc_id'])\n",
    "#         print (self.test_df[self.test_df['query'] == self.query]['doc_id'].iloc[0] in \n",
    "#                [response[0] for response in sorted_responses[:1]])\n",
    "        return [response[0] for response in sorted_responses[:1]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "#         self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "#         acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    model_list = three_groups_list\n",
    "\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "        thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4562c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING QUERIES FOUND TO CONFUSE MODELS IN ALL SHARDS:\n",
    "# query = 'how to create search box in excel'\n",
    "# for group in three_groups_list:\n",
    "#     models = three_groups_list[group]\n",
    "#     print (group)\n",
    "#     for model in models:\n",
    "        \n",
    "#         input_ids = tokenizer.encode(query, return_tensors='pt')\n",
    "#         output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "#                                     num_beams=1, num_return_sequences=1, max_length = 20)\n",
    "#         model_res = [tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "#         print (globals()[f'test_df_{group}'][globals()[f'test_df_{group}'][\"doc_id\"] == model_res[0]]['doc'].iloc[0])\n",
    "#         print (model_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f86061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_samplingmodels.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_samplingmodels.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d88a2",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 5 beams, with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f25364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 1\n",
      "peer 1\n",
      "peer 2\n",
      "peer 3\n",
      "peer 4\n",
      "peer 5\n",
      "peer 6\n",
      "peer 7\n",
      "peer 8\n",
      "peer 9\n",
      "peer 10\n",
      "group 2\n",
      "peer 1\n",
      "peer 2\n",
      "peer 3\n",
      "peer 4\n",
      "peer 5\n",
      "peer 6\n",
      "peer 7\n",
      "peer 8\n",
      "peer 9\n",
      "peer 10\n",
      "group 3\n",
      "peer 1\n",
      "peer 2\n",
      "peer 3\n",
      "peer 4\n",
      "peer 5\n",
      "peer 6\n",
      "peer 7\n",
      "peer 8\n",
      "peer 9\n",
      "peer 10\n"
     ]
    }
   ],
   "source": [
    "three_groups_list = defaultdict(list)\n",
    "for group in groups:\n",
    "    print (f'group {group}')\n",
    "    for i, peer in enumerate(peers):\n",
    "        print (f'peer {int(peer) - 8089}')\n",
    "        exec(f'three_groups_list[\"group{group}\"].append(model_group{group}_peer{int(peer)-8089})')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5eaaf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size:test set size: 17329\n",
      " 16464\n",
      "test set size: 16674\n",
      "Processed 500 queries\n",
      "Processed 500 queries\n",
      "Processed 500 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1500 queries\n",
      "Processed 1500 queries\n",
      "Processed 1500 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2500 queries\n",
      "Processed 2500 queries\n",
      "Processed 2500 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3500 queries\n",
      "Processed 3500 queries\n",
      "Processed 3500 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4500 queries\n",
      "Processed 4500 queries\n",
      "Processed 4500 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5500 queries\n",
      "Processed 5500 queries\n",
      "Processed 5500 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6500 queries\n",
      "Processed 6500 queries\n",
      "Processed 6500 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7500 queries\n",
      "Processed 7500 queries\n",
      "Processed 7500 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8500 queries\n",
      "Processed 8500 queries\n",
      "Processed 8500 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9500 queries\n",
      "Processed 9500 queries\n",
      "Processed 9500 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10500 queries\n",
      "Processed 10500 queries\n",
      "Processed 10500 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11500 queries\n",
      "Processed 11500 queries\n",
      "Processed 11500 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12500 queries\n",
      "Processed 12500 queries\n",
      "Processed 12500 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13500 queries\n",
      "Processed 13500 queries\n",
      "Processed 13500 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14500 queries\n",
      "Processed 14500 queries\n",
      "Processed 14500 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15500 queries\n",
      "Processed 15500 queries\n",
      "Processed 15500 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16500 queries\n",
      "Processed 16500 queries\n",
      "Group: 1, Test Acc: 0.934645286686103\n",
      "Group: 3, Test Acc: 0.9411658870097157\n",
      "Processed 17000 queries\n",
      "Group: 2, Test Acc: 0.9309250389520457\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.test_df = test_df.copy()\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 500 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        \n",
    "        sampled_models = random.sample(self.model_list['group1'], 3)\n",
    "        sampled_models.extend(random.sample(self.model_list['group2'], 3))\n",
    "        sampled_models.extend(random.sample(self.model_list['group3'], 3))\n",
    "#         print ('new query')\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5, max_length = 20)\n",
    "\n",
    "            beam_scores = output.sequences_scores\n",
    "#             print (beam_scores)\n",
    "            probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "#             print (probabilities)\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            \n",
    "\n",
    "            for res, prob in zip(model_res, probabilities):\n",
    "                results[res] += prob\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:5]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "#         self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "#         acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    model_list = three_groups_list\n",
    "\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1,4):\n",
    "        thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420481ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.8048935310750764\n",
      "1 0.793488824101069\n",
      "3 0.8219983207388749\n"
     ]
    }
   ],
   "source": [
    "for group in model_managers:\n",
    "    df_temp = model_managers[group].test_df.copy()\n",
    "    df_temp['generated_doc_id_1beam'] = df_temp['generated_doc_id'].apply(lambda x: [x[0]])\n",
    "#     regular_dict2[group] = df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "#                                 axis=1).sum() / df_temp.shape[0]\n",
    "    print (group, df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "                                axis=1).sum() / df_temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be52d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.8496162502164003\n",
      "1 0.8482142857142857\n",
      "3 0.8720163128223581\n"
     ]
    }
   ],
   "source": [
    "for group in model_managers:\n",
    "    df_temp = model_managers[group].test_df.copy()\n",
    "    df_temp['generated_doc_id_1beam'] = df_temp['generated_doc_id'].apply(lambda x: [x[0]])\n",
    "#     regular_dict2[group] = df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "#                                 axis=1).sum() / df_temp.shape[0]\n",
    "    print (group, df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "                                axis=1).sum() / df_temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70580685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_samplingmodels_5beams_probabilities.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e36e2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'acc_test': 0.9327016520894071},\n",
       " 3: {'acc_test': 0.9405061772819959},\n",
       " 2: {'acc_test': 0.9316175197645565}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('global_accuracies_samplingmodels_5beams_probabilities.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_dict2 = {}\n",
    "for group in model_managers:\n",
    "    df_temp = model_managers[group].test_df.copy()\n",
    "    df_temp['generated_doc_id_1beam'] = df_temp['generated_doc_id'].apply(lambda x: [x[0]])\n",
    "    regular_dict2[group] = df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "                                axis=1).sum() / df_temp.shape[0]\n",
    "    print (group, df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "                                axis=1).sum() / df_temp.shape[0])\n",
    "    \n",
    "    \n",
    "# display(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_samplingmodels_probabilities.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f561b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.8495008367476484, 1: 0.8486394557823129, 3: 0.8718363919875255}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('global_accuracies_samplingmodels_probabilities.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c24dff",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 1 beams, with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_groups_list = defaultdict(list)\n",
    "for group in groups:\n",
    "    print (f'group {group}')\n",
    "    for i, peer in enumerate(peers):\n",
    "        print (f'peer {int(peer) - 8089}')\n",
    "        exec(f'three_groups_list[\"group{group}\"].append(model_group{group}_peer{int(peer)-8089})')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72e19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.test_df = test_df.copy()\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 500 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        \n",
    "        sampled_models = random.sample(self.model_list['group1'], 3)\n",
    "        sampled_models.extend(random.sample(self.model_list['group2'], 3))\n",
    "        sampled_models.extend(random.sample(self.model_list['group3'], 3))\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5, max_length = 20)\n",
    "\n",
    "            beam_scores = output.sequences_scores\n",
    "            probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            \n",
    "\n",
    "            for res, prob in zip(model_res, probabilities):\n",
    "                results[res] += prob\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:1]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "#         self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "#         acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    model_list = three_groups_list\n",
    "\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "        thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b258cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_samplingmodels_probabilities.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34790e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_samplingmodels_probabilities.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb3d382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7b0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
